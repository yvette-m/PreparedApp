{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Our LLM Model**\n",
        "\n",
        "One of the first things we had to do was figure out what LLM we wanted to use. We chose Ollama over GPT and Gemini because it allows full local hosting of its model, offering free and flexible usage without API charges.\n",
        "\n",
        "Here we take the time to download Ollama on the host. Ollama can be remotely hosted on Google Colab thanks to the build in GPU runtime options."
      ],
      "metadata": {
        "id": "rDW0HUVHivsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install ollama on the host\n",
        "!curl https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "id": "Sm2H34A0fTte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama\n"
      ],
      "metadata": {
        "id": "LPd6rallfVzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Introduction of Ngrok:**\n",
        "\n",
        " “Ngrok is a tool that creates a secure tunnel from the public internet to your local machine, allowing your app to be accessed remotely. It’s useful when deploying apps or services that are running on your local machine but need to be accessed over the internet.”"
      ],
      "metadata": {
        "id": "poNiMAFujL18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # install dependencies for python script\n",
        "!pip install aiohttp pyngrok\n",
        "\n",
        "!ngrok config add-authtoken 2p5IC1UVZyFzKJwqRJMwCAR3HZX_6BgDQLWbWnrjGX6A3xQhW\n"
      ],
      "metadata": {
        "id": "I83zBrTLjLGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " **How does this code work?**\n",
        "\n",
        " Ngrok basically allowed us to expose the remote Ollama instance to the public. After running this code block\n",
        " 1. Ollama mistral will begin running\n",
        " 2. A URL is generated that can be shared as a means to utilize the service (Ollama mistral running remotely)\n",
        "  meaning we could now access the service from within VScode where the app is running.\n",
        "\n",
        "We also took the liberty to automate running Ollama's mistral model here.\n",
        "\n",
        "In order for this code to work - the main user requires a Ngrok account. Ngrok made our code possible but for a more professional development an investment into using AWS or another cloud hosting platform is necessary.\n",
        "\n",
        "Current Authentication Token is from Yvette Roos from Tufts University."
      ],
      "metadata": {
        "id": "ZgJJt77ChmfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "async def run_process(cmd):\n",
        "  print('>>> starting', *cmd)\n",
        "  p = await asyncio.subprocess.create_subprocess_exec(\n",
        "      *cmd,\n",
        "      stdout=asyncio.subprocess.PIPE,\n",
        "      stderr=asyncio.subprocess.PIPE,\n",
        "  )\n",
        "\n",
        "  async def pipe(lines):\n",
        "    async for line in lines:\n",
        "      print(line.strip().decode('utf-8'))\n",
        "\n",
        "  await asyncio.gather(\n",
        "      pipe(p.stdout),\n",
        "      pipe(p.stderr),\n",
        "  )\n",
        "\n",
        "#register an account at ngrok.com and create an authtoken and place it here\n",
        "await asyncio.gather(\n",
        "    run_process(['ngrok', 'config', 'add-authtoken','2p5IC1UVZyFzKJwqRJMwCAR3HZX_6BgDQLWbWnrjGX6A3xQhW'])\n",
        ")\n",
        "\n",
        "await asyncio.gather(\n",
        "    run_process(['ollama', 'serve']),\n",
        "    run_process(['ollama', 'run', 'mistral']),  # Run Ollama Mistral command\n",
        "    run_process(['ngrok', 'http', '--log', 'stderr', '11434', '--host-header', 'localhost:11434'])\n",
        ")\n"
      ],
      "metadata": {
        "id": "xWZu3juyfZd-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}